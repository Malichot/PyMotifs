{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84800f59",
   "metadata": {},
   "source": [
    "# PyMotifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e1495",
   "metadata": {},
   "source": [
    "## Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3c861",
   "metadata": {},
   "source": [
    "Le présent script permet l'annotation d'un enesemble de fichiers txt présents dans un dossier. Il prend des fichiers .txt présents dans votre dossier, les met dans un tableau (dataframe), tokenise les textes et les annote (lemmatisation, pos-tagging, morphologie). La fonction utilise l'annotateur Sapcy-UDPipe et télécharge le modèle d'annotation si vous ne l'avez pas déjà.\n",
    "\n",
    "_Paramètres :_\n",
    "\n",
    "- path_txt = path to your folder containing your texts .txt. Be careful that no hidden files are in the folder. \n",
    "    - Ex : \"~/Users/Desktop/PyMotifs/corpus\"\n",
    "- save_csv = path your folder to save the generated csv and name of the csv.\n",
    "    - Ex : \"~/Users/Desktop/PyMotifs/output/corpus_annotated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5664fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import csv\n",
    "import spacy_udpipe\n",
    "import re\n",
    "# !pip install spacy-udpipe\n",
    "\n",
    "def annotate(path_txt, save_path):\n",
    "    \"\"\"\n",
    "    Function that takes .txt files, put them into a dataframe, tokenizes texts and annotate them. \n",
    "    Download the annotation model if you don't already have it.\n",
    "    Uses Sapcy UDPipe annotator and do : lemmatization, pos-tagging, morphology.\n",
    "    \n",
    "    Parameters: \n",
    "    path_txt = path to your folder containing your texts .txt. Be careful that no hidden files are in the folder. \n",
    "        Ex : \"~/Users/Desktop/PyMotifs/corpus\"\n",
    "    save_csv = path your folder to save the generated csv and name of the csv.\n",
    "        Ex : \"~/Users/Desktop/PyMotifs/output/corpus_annotated.csv\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Loop over : https://stackoverflow.com/questions/69118811/how-to-read-all-txt-files-from-a-directory\n",
    "    def read_txts(path):\n",
    "        \"\"\"\n",
    "        Read all txts inside a folder and put them into a dataframe.\n",
    "        Folder must contain only txt files...\n",
    "        \"\"\"\n",
    "        files_content = [] # create empty list to save content\n",
    "\n",
    "        for filename in filter(lambda p: p.endswith(\"txt\"), os.listdir(path)): # filtre les fichiers qui se terminent par txt\n",
    "          # et liste les fichiers dans le path.\n",
    "            filepath = os.path.join(path, filename)\n",
    "            with open(filepath, mode='r') as f:\n",
    "                files_content += [f.read()]\n",
    "\n",
    "        print(f'There are {len(files_content)} texts in folder')\n",
    "        all_files = os.listdir(path=path)\n",
    "        df = pd.DataFrame()\n",
    "        df['filename'] = all_files\n",
    "        df['text'] = files_content\n",
    "        return(df)\n",
    "\n",
    "    read_txts(path=path_txt)\n",
    "\n",
    "    # Save into object : \n",
    "    df = read_txts(path=path_txt)\n",
    "\n",
    "    # Change apostrophs : \n",
    "\n",
    "    def clean_a_bit(df):\n",
    "        \"\"\"\n",
    "        Function to clean differnt apostrophs and withdraw possible na\n",
    "        values from df\n",
    "        \"\"\"\n",
    "        df['text'] = df['text'].replace(\"’\", \"'\")\n",
    "        df['text'] = df['text'].replace(\"'\", \"'\")\n",
    "        # Retrait des NA dans la colonne mots : \n",
    "        df['text'] = df['text'].dropna(how = 'any', axis = 0)# Drop the row \n",
    "\n",
    "        return(df)\n",
    "\n",
    "    clean_a_bit(df)\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "\n",
    "    # Annotation : \n",
    "\n",
    "    spacy_udpipe.download(\"fr\") # Download french model : \n",
    "\n",
    "    nlp = spacy_udpipe.load(\"fr\")\n",
    "\n",
    "    # Create a Tokenizer with the default settings for French\n",
    "    # including punctuation rules and exceptions\n",
    "\n",
    "    tokenizer = nlp.tokenizer\n",
    "\n",
    "    # Création d'une nouvelle dataframe pour accueillir les données de l'étiquetage : \n",
    "    # On ne veut pas garder le texte intégral dans le nouveau tableau.\n",
    "\n",
    "    annotated_datas = pd.DataFrame()\n",
    "\n",
    "    # récupération de la colonne filename : \n",
    "\n",
    "    annotated_datas['filename'] = df['filename']\n",
    "\n",
    "    # On tokenise les textes :\n",
    "\n",
    "    annotated_datas['words'] = df['text'].apply(lambda x: nlp.tokenizer(str(x)))\n",
    "    annotated_datas.head(10)\n",
    "\n",
    "    # \"Explostion\" des données : un mot par ligne : \n",
    "    annotated_datas = annotated_datas.explode(\"words\", ignore_index=True)\n",
    "    annotated_datas.head(10)\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "\n",
    "    # Étiquetage et lemmatisation : \n",
    "\n",
    "    ## Thx to Ed Rushton :\n",
    "    # Cf. https://stackoverflow.com/questions/44395656/applying-spacy-parser-to-pandas-dataframe-w-multiprocessing\n",
    "\n",
    "    ## Spacy is highly optimised and does the multiprocessing for you. \n",
    "    ## As a result, I think your best bet is to take the data out of \n",
    "    ## the Dataframe and pass it to the Spacy pipeline as a list rather \n",
    "    ## than trying to use .apply directly.\n",
    "    ## You then need to the collate the results of the parse, and put \n",
    "    ## this back into the Dataframe. \n",
    "\n",
    "    lemma = []\n",
    "    pos = []\n",
    "    morph = []\n",
    "    dep = []\n",
    "\n",
    "    for doc in nlp.pipe(annotated_datas['words'].astype('unicode').values, batch_size=50):\n",
    "        if doc.has_annotation:\n",
    "            #tokens.append([n.text for n in doc])\n",
    "            lemma.append([n.lemma_ for n in doc])\n",
    "            pos.append([n.pos_ for n in doc])\n",
    "            morph.append([n.morph for n in doc])\n",
    "            dep.append([n.dep_ for n in doc])\n",
    "        else:\n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "            # tokens.append(None)\n",
    "            lemma.append(None)\n",
    "            pos.append(None)\n",
    "            morph.append(None)\n",
    "\n",
    "    # corpus_test['tokens'] = tokens\n",
    "    annotated_datas['lemma'] = lemma\n",
    "    annotated_datas['pos'] = pos\n",
    "    annotated_datas['morph'] = morph\n",
    "    print(annotated_datas.head())\n",
    "    \n",
    "    \n",
    "    # Explosion des données : \n",
    "    annotated_datas = annotated_datas.explode(\"words\", ignore_index=True)\n",
    "    #annotated_datas = annotated_datas.explode(\"pos\", ignore_index=True)\n",
    "    #annotated_datas = annotated_datas.explode(\"lemma\", ignore_index=True)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    \n",
    "    # Saving into csv : \n",
    "\n",
    "    annotated_datas.to_csv(path_or_buf=save_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "433472df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 texts in folder\n",
      "There are 2 texts in folder\n",
      "Already downloaded a model for the 'fr' language\n",
      "       filename  words    lemma     pos  \\\n",
      "0  Huysmans.txt      À      [à]   [ADP]   \n",
      "1  Huysmans.txt     en     [en]   [ADP]   \n",
      "2  Huysmans.txt  juger  [juger]  [VERB]   \n",
      "3  Huysmans.txt    par    [par]   [ADP]   \n",
      "4  Huysmans.txt    les     [le]   [DET]   \n",
      "\n",
      "                                         morph  \n",
      "0                                         [()]  \n",
      "1                                         [()]  \n",
      "2                             [(VerbForm=Inf)]  \n",
      "3                                         [()]  \n",
      "4  [(Definite=Def, Number=Plur, PronType=Art)]  \n"
     ]
    }
   ],
   "source": [
    "annotate(path_txt= \"/Users/adesacy/Desktop/PyMotifs/corpus/\", \n",
    "         save_path=\"/Users/adesacy/Desktop/PyMotifs/output/corpus_annotated_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
